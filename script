"""
Author: Nicole Wang
Date: Updated June 2025

Bayesian-Cognitive-Modeling:
This script estimates the cognitive parameter W using Bayesian inference and the Metropolis algorithm
on real psychophysical task data. W represents perceptual noise in decision-making.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import norm
from math import exp, log

# ------------------------------
# Load dataset
# ------------------------------

df = pd.read_csv("data.csv")  # Ensure this file exists in your repo
a = df['a'].values
n1 = df['n1'].values
n2 = df['n2'].values

# ------------------------------
# Model definitions
# ------------------------------

def log_prior(W):
    """Log prior probability of W using exponential distribution."""
    if W <= 0:
        return -np.inf
    return -W  # log(exp(-W)) = -W

def log_likelihood(n1, n2, a, W):
    """Log likelihood of responses given W."""
    assert len(n1) == len(n2) == len(a)
    p = 1.0 - norm.cdf(0, loc=np.abs(n1 - n2), scale=W * np.sqrt(n1**2 + n2**2))
    return np.sum(np.where(a, np.log(p), np.log(1.0 - p)))

def log_posterior(n1, n2, a, W):
    """Log posterior as sum of log-prior and log-likelihood."""
    return log_prior(W) + log_likelihood(n1, n2, a, W)

# ------------------------------
# Metropolis sampling
# ------------------------------

def metropolis_sampler(log_post_fn, n1, n2, a, initial_W=0.6, steps=10000, burn_in=1000, step_size=0.1):
    """Run Metropolis algorithm to sample from posterior."""
    W = initial_W
    samples = []
    for _ in range(steps):
        W_prop = W + np.random.normal(0, step_size)
        log_post_curr = log_post_fn(n1, n2, a, W)
        log_post_prop = log_post_fn(n1, n2, a, W_prop)
        ratio = exp(log_post_prop - log_post_curr)
        if ratio > 1 or np.random.rand() < ratio:
            W = W_prop
        samples.append(W)
    return np.array(samples[burn_in:])  # Discard burn-in

# ------------------------------
# Posterior analysis
# ------------------------------

posterior_samples = metropolis_sampler(log_posterior, n1, n2, a)

# Plot posterior histogram
plt.figure(figsize=(8, 4))
plt.hist(posterior_samples, bins=100, density=True, alpha=0.7)
plt.title("Posterior Distribution of W")
plt.xlabel("W")
plt.ylabel("Density")
plt.grid(True)
plt.tight_layout()
plt.show()

# Compute probability W ∈ [0.60, 0.65]
prob_interval = np.mean((posterior_samples >= 0.6) & (posterior_samples <= 0.65))
print(f"Probability that W ∈ [0.60, 0.65]: {prob_interval:.3f}")

# ------------------------------
# Prior sampling for comparison
# ------------------------------

def sample_prior(log_prior_fn, initial_W=0.6, steps=10000, step_size=0.1):
    """Run Metropolis algorithm on prior alone."""
    W = initial_W
    samples = []
    for _ in range(steps):
        W_prop = W + np.random.normal(0, step_size)
        log_prior_curr = log_prior_fn(W)
        log_prior_prop = log_prior_fn(W_prop)
        ratio = exp(log_prior_prop - log_prior_curr)
        if ratio > 1 or np.random.rand() < ratio:
            W = W_prop
        samples.append(W)
    return np.array(samples)

prior_samples = sample_prior(log_prior)

# Plot prior vs posterior
plt.figure(figsize=(8, 4))
plt.hist(posterior_samples, bins=100, density=True, alpha=0.6, label="Posterior")
plt.hist(prior_samples, bins=100, density=True, alpha=0.6, label="Prior")
plt.title("Prior vs Posterior Distribution of W")
plt.xlabel("W")
plt.ylabel("Density")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
